{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2a11a91e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary libraries\n",
    "from wordcloud import WordCloud\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from IPython.display import Image, display\n",
    "from docx import Document\n",
    "from sumy.parsers.plaintext import PlaintextParser\n",
    "from sumy.nlp.tokenizers import Tokenizer\n",
    "from sumy.summarizers.text_rank import TextRankSummarizer\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "16af7995",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to summarize text using Gensim TextRank\n",
    "def summarize_text_textrank(text, sentences_count=5):\n",
    "    parser = PlaintextParser.from_string(text, Tokenizer(\"english\"))\n",
    "    summarizer = TextRankSummarizer()\n",
    "    summary = summarizer(parser.document, sentences_count)\n",
    "    return \" \".join([str(sentence) for sentence in summary])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7d3c77be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract text from a Word document\n",
    "def extract_text_from_docx(file_path):\n",
    "    doc = Document(file_path)\n",
    "    text = '\\n'.join([paragraph.text for paragraph in doc.paragraphs])\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "092700e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate a word cloud\n",
    "def generate_word_cloud(text):\n",
    "    wordcloud = WordCloud(width=800, height=400, background_color='white').generate(text)\n",
    "    return wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0080a3a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to analyze sentiment using NLTK's VADER\n",
    "def analyze_sentiment(text):\n",
    "    sia = SentimentIntensityAnalyzer()\n",
    "    sentiment = sia.polarity_scores(text)\n",
    "    return sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0b5a53a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Select a summarization library (textrank): spaCy\n"
     ]
    }
   ],
   "source": [
    "# Let the user select the summarization library\n",
    "summarization_library = input(\"Select a summarization library (textrank): \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2a61e3e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter 'text' to input text or 'file' to specify a file path: Any change that deliberately compromises Wikipedia's integrity is considered vandalism. The most common and obvious types of vandalism include additions of obscenities and crude humor; it can also include advertising and other types of spam.[93] Sometimes editors commit vandalism by removing content or entirely blanking a given page. Less common types of vandalism, such as the deliberate addition of plausible but false information, can be more difficult to detect. Vandals can introduce irrelevant formatting, modify page semantics such as the page's title or categorization, manipulate the article's underlying code, or use images disruptively.[94]  White-haired elderly gentleman in suit and tie speaks at a podium. American journalist John Seigenthaler (1927â€“2014), subject of the Seigenthaler incident Obvious vandalism is generally easy to remove from Wikipedia articles; the median time to detect and fix it is a few minutes.[95][96] However, some vandalism takes much longer to detect and repair.[97]  In the Seigenthaler biography incident, an anonymous editor introduced false information into the biography of American political figure John Seigenthaler in May 2005, falsely presenting him as a suspect in the assassination of John F. Kennedy.[97] It remained uncorrected for four months.[97] Seigenthaler, the founding editorial director of USA Today and founder of the Freedom Forum First Amendment Center at Vanderbilt University, called Wikipedia co-founder Jimmy Wales and asked whether he had any way of knowing who contributed the misinformation. Wales said he did not, although the perpetrator was eventually traced.[98][99] After the incident, Seigenthaler described Wikipedia as \"a flawed and irresponsible research tool\".[97] The incident led to policy changes at Wikipedia for tightening up the verifiability of biographical articles of living people.\n"
     ]
    }
   ],
   "source": [
    "# Input text or specify a file path\n",
    "source = input(\"Enter 'text' to input text or 'file' to specify a file path: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e194fa56",
   "metadata": {},
   "outputs": [],
   "source": [
    "if source == 'text':\n",
    "    input_text = input(\"Enter the text you want to summarize: \")\n",
    "    text_to_summarize = input_text\n",
    "else:\n",
    "    file_path = input(\"Enter the path to the Word document: \")\n",
    "    if file_path.endswith('.docx'):\n",
    "        text_to_summarize = extract_text_from_docx(file_path)\n",
    "    else:\n",
    "        print(\"Unsupported file format. Please provide a .docx file.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d65ad1f5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
